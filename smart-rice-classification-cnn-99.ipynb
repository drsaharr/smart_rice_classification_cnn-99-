{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":31192,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"<div style=\"color: red; padding: 10px; text-align: center; height: 70px;\">\n    <div style=\"\n        margin: auto;\n        width: 90%;\n        border-top-left-radius: 25px;\n        background: url('https://winfogroup.ir/wp-content/uploads/2025/09/%D8%B1%D9%88%D8%B4-%D9%BE%D8%AE%D8%AA-%D8%A8%D8%B1%D9%86%D8%AC-1.jpg') no-repeat center center;\n        border-bottom-right-radius: 25px;\n        border-top-right-radius: 50px;\n        border-bottom-left-radius: 50px;\n        color: red;\n        height: 100%;\n        font-size: 28px;\n        line-height: 70px;\n        box-shadow: 0 0 5px rgb(0, 103, 15), 0 0 15px rgba(0, 255, 255, 0.5);\n        font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;\n        text-shadow: 1px 1px 3px black;\n    \">\n        Rice Classification\n    </div>\n</div>\n","metadata":{}},{"cell_type":"markdown","source":"<div style=\"text-align:center;\">\n    <img src=\"https://nutritionsource.hsph.harvard.edu/wp-content/uploads/2024/11/AdobeStock_195457011-1024x683.jpeg\" \n         alt=\"ISAAA image\" \n         style=\"max-width:90%; height:auto; border:2px solid #ccc; border-radius:10px;\">\n</div>\n","metadata":{}},{"cell_type":"markdown","source":"<div style=\"box-sizing:border-box;font-family: 'Segoe UI', Roboto, Arial, sans-serif; padding:22px; max-width:900px; margin:20px auto; border-radius:18px; background:linear-gradient(135deg,#ffffff 0%,#f1f5f9 50%,#e2e8f0 100%); box-shadow:0 10px 35px rgba(15,23,42,0.12); border:1px solid rgba(15,23,42,0.06);\">\n\n  <header style=\"display:flex;align-items:center;gap:14px;margin-bottom:16px;\">\n    <div style=\"width:40px;height:60px;border-radius:14px;flex:0 0 60px;background:linear-gradient(180deg,#2563eb,#9333ea);display:flex;align-items:center;justify-content:center;color:white;font-weight:700;font-size:20px;\">\n      RI\n    </div>\n    <div>\n      <h1 style=\"margin:0;font-size:22px;color:#0f172a;\">Abstract</h1>\n      <div style=\"font-size:13px;color:#475569;margin-top:4px;\">Rice Seed Classification</div>\n    </div>\n  </header>\n\n  <section style=\"background:rgba(255,255,255,0.7); padding:16px; border-radius:12px; border:1px solid rgba(2,6,23,0.04); line-height:1.7; color:#0f172a; font-size:15px;\">\n    Rice, which is among the most widely produced grain products worldwide, has many genetic varieties. These varieties are separated from each other due to some of their features. These are usually features such as texture, shape, and color. With these features that distinguish rice varieties, it is possible to classify and evaluate the quality of seeds. In this study, Arborio, Basmati, Ipsala, Jasmine and Karacadag, which are five different varieties of rice often grown in Turkey, were used. A total of 75,000 grain images, 15,000 from each of these varieties, are included in the dataset. A second dataset with 106 features including 12 morphological, 4 shape and 90 color features obtained from these images was used. Models were created by using Artificial Neural Network (ANN) and Deep Neural Network (DNN) algorithms for the feature dataset and by using the Convolutional Neural Network (CNN) algorithm for the image dataset, and classification processes were performed. Statistical results of sensitivity, specificity, prediction, F1 score, accuracy, false positive rate and false negative rate were calculated using the confusion matrix values of the models and the results of each model were given in tables.\n  </section>\n\n</div>\n","metadata":{}},{"cell_type":"markdown","source":"\n \n<div style=\"\n    background: black;\n    width: 30%;\n    margin: 30px auto;\n    border-radius: 20px;\n    text-align: center;\n    box-shadow: 0 0 5px rgba(255, 0, 0, 0.77);\n    color:rgb(161, 13, 13);\n    font-size: 26px;\n    padding: 20px 30px;\n    font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;\n    font-weight: bold;\n\">\n  import library\n</div>\n","metadata":{}},{"cell_type":"code","source":"import os\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers, models\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nimport matplotlib.pyplot as plt\nimport cv2\nimport random\nimport seaborn as sns \nfrom termcolor import colored  \nimport pathlib\nimport itertools\n# import splitfolders\nfrom sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay,classification_report\nimport warnings\nwarnings.filterwarnings('ignore')\nimport tensorflow as tf\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Conv2D, MaxPooling2D, Dense, Flatten, BatchNormalization,Dropout\nfrom IPython.display import HTML, display\n","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"\n \n<div style=\"\n    background: black;\n    width: 30%;\n    margin: 30px auto;\n    border-radius: 20px;\n    text-align: center;\n    box-shadow: 0 0 5px rgba(255, 0, 0, 0.77);\n    color:rgb(161, 13, 13);\n    font-size: 26px;\n    padding: 20px 30px;\n    font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;\n    font-weight: bold;\n\">\n Import Dataset\n</div>\n","metadata":{}},{"cell_type":"code","source":"dir1 = '/kaggle/input/rice-image-dataset/Rice_Image_Dataset'\n\ndetection = [class_name for class_name in os.listdir(dir1) if class_name]\ndetection.remove('Rice_Citation_Request.txt')\n","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"<div style=\"\n  width: 80%;\n  max-width: 700px;\n  margin: 30px auto;\n  padding: 40px 0;\n  text-align: center;\n  font-size: 40px;\n  font-weight: bold;\n  color: white;\n  background: linear-gradient(135deg, #00ffc3, #7a00ff);\n  border-radius: 20px;\n  box-shadow: 0 0 25px rgba(0,255,200,0.5);\n\">\n  EDA\n</div>\n","metadata":{}},{"cell_type":"markdown","source":"\n \n<div style=\"\n    background: black;\n    width: 30%;\n    margin: 30px auto;\n    border-radius: 20px;\n    text-align: center;\n    box-shadow: 0 0 5px rgba(255, 0, 0, 0.77);\n    color:rgb(161, 13, 13);\n    font-size: 26px;\n    padding: 20px 30px;\n    font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;\n    font-weight: bold;\n\">\n Class name\n</div>\n","metadata":{}},{"cell_type":"code","source":"\n\nhtml_content = f\"\"\"\n<div style=\"\n  width: 90%;\n  max-width: 700px;\n  margin: 30px auto;\n  padding: 20px;\n  background: #111;\n  border-radius: 15px;\n  color: #0ff;\n  font-size: 18px;\n  box-shadow: 0 0 20px rgba(0,255,255,0.4);\n\">\n  <div style='font-weight:bold; margin-bottom:12px; font-size:22px;'>DETECTION CLASSES</div>\n\n  <div style=\"display:flex; flex-wrap:wrap; gap:10px;\">\n\"\"\"\n\nfor cls in detection:\n    html_content += f\"\"\"\n    <div style=\"\n      padding: 8px 14px;\n      background: #0ff3;\n      border-radius: 8px;\n    \">{cls}</div>\n    \"\"\"\n\nhtml_content += \"</div></div>\"\n\ndisplay(HTML(html_content))\n","metadata":{"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"\n \n<div style=\"\n    background: black;\n    width: 30%;\n    margin: 30px auto;\n    border-radius: 20px;\n    text-align: center;\n    box-shadow: 0 0 5px rgba(255, 0, 0, 0.77);\n    color:rgb(161, 13, 13);\n    font-size: 26px;\n    padding: 20px 30px;\n    font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;\n    font-weight: bold;\n\">\n Class photos\n</div>\n","metadata":{}},{"cell_type":"code","source":"\nimport random\n\nrows = 1\ncols = 5\nn = rows * cols\n\nfor x in detection:\n    folder = os.path.join(dir1, x)\n    images = os.listdir(folder)\n\n    fig, ax = plt.subplots(rows, cols, figsize=(15, 4))\n    fig.suptitle(x, fontsize=16, weight=\"bold\")\n\n \n    if rows == 1 and cols == 1:\n        ax = [ax]\n    elif rows == 1:\n        ax = ax.flatten().tolist()\n    else:\n        ax = ax.reshape(-1).tolist()\n\n\n    selected_images = random.sample(images, n)\n\n    for i in range(n):\n        img_path = os.path.join(folder, selected_images[i])\n        img = cv2.imread(img_path)\n        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n\n        ax[i].imshow(img)\n        ax[i].set_title(selected_images[i], fontsize=9)\n        ax[i].axis('off')\n\n        h, w = img.shape[:2]\n        center_x = w // 2\n        center_y = h // 2\n        radius = min(h, w) // 2.5\n\n        circle = plt.Circle(\n            (center_x, center_y),\n            radius,\n            fill=False,\n            linewidth=4,\n            edgecolor=\"blue\"\n        )\n        ax[i].add_patch(circle)\n\n    plt.tight_layout()\n    plt.show()\n","metadata":{"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# lets examine each type of rice to see its exact shape?","metadata":{}},{"cell_type":"markdown","source":"\n\n<div style=\"padding: 10px; text-align: center; height: 70px;\">\n    <div style=\"\n        padding: 0 50px;\n        margin: auto;\n        width: fit-content;\n        border-radius: 25px 50px 50px 25px;\n        background: black;\n        color: #ffffff;\n        height: 90%;\n        font-size: 30px;\n        line-height: 70px;\n        box-shadow: 0 0 8px green;\n        font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;\n        text-shadow: 1px 1px 3px black;\n    \">\n        Ipsala\n    </div>\n</div>\n\n","metadata":{}},{"cell_type":"code","source":"\nfolder = os.path.join(dir1, 'Ipsala') \n\n\nimages = [f for f in os.listdir(folder)]\n\nn_rows, n_cols = 2, 3\nn = n_rows * n_cols\n\nselected_images = random.sample(images, n)\n\nfig, ax = plt.subplots(n_rows, n_cols, figsize=(15, 7))\nfig.suptitle('Ipsala', fontsize=18)\n\n# ŸÜŸÖÿß€åÿ¥ ÿπ⁄©ÿ≥‚ÄåŸáÿß ÿØÿ± ÿ¥ÿ®⁄©Ÿá\nfor idx, img_name in enumerate(selected_images):\n    row = idx // n_cols\n    col = idx % n_cols\n    img_path = os.path.join(folder, img_name)\n    img = cv2.imread(img_path)\n    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n    ax[row][col].imshow(img)\n    ax[row][col].set_title(img_name, fontsize=7)\n    ax[row][col].axis('off')\n\n\nplt.tight_layout()\nplt.show()\n","metadata":{"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"<div style=\"color: #111; background: #cfd3daff; border-radius: 20px; max-width: 960px; margin: 0 auto; box-shadow: 0 6px 20px rgba(15,23,42,0.08); padding: 32px; border: 1px solid #e6e6e6;\">\n  <!-- Grain Summary Box -->\n  <div style=\"background: #fff7e6; border-radius: 14px; padding: 22px; box-shadow: 0 6px 20px rgba(15,23,42,0.08); border: 1px solid #e6e6e6; margin-bottom: 20px;\">\n    <h1 style=\"font-size: 22px; border-bottom: 2px solid #facc15; display: inline-block; padding-bottom: 4px;\">\n      Grain Shape Summary ‚Äî <strong>Ipsala</strong>\n    </h1>\n    <p style=\"margin: 6px 0 18px; color: #6b7280; font-size: 15px; line-height: 1.6;\">\n      A concise overview of the physical and visual characteristics of Ipsala rice grains, compiled from scientific sources for better understanding.\n    </p>\n    <table style=\"width: 100%; border-collapse: collapse; margin-top: 10px; font-size: 14px; border-radius: 10px; overflow: hidden;\">\n      <tr>\n        <th style=\"background: #fdfbf6; color: #374151; font-weight: 600; padding: 10px 12px; border-bottom: 1px solid #e6e6e6;\">Feature</th>\n        <th style=\"background: #fdfbf6; color: #374151; font-weight: 600; padding: 10px 12px; border-bottom: 1px solid #e6e6e6;\">Exact Value</th>\n        <th style=\"background: #fdfbf6; color: #374151; font-weight: 600; padding: 10px 12px; border-bottom: 1px solid #e6e6e6;\">Grain Type</th>\n      </tr>\n      <tr>\n        <td style=\"padding: 10px 12px; border-bottom: 1px dashed #e5e7eb;\">Overall Shape</td>\n        <td style=\"padding: 10px 12px; border-bottom: 1px dashed #e5e7eb;\">Elliptical / Oval</td>\n        <td rowspan=\"6\" style=\"padding: 10px 12px; border-bottom: 1px dashed #e5e7eb;\">Medium-Grain</td>\n      </tr>\n      <tr>\n        <td style=\"padding: 10px 12px; border-bottom: 1px dashed #e5e7eb;\">Grain Tip</td>\n        <td style=\"padding: 10px 12px; border-bottom: 1px dashed #e5e7eb;\">Slightly Curved</td>\n      </tr>\n      <tr>\n        <td style=\"padding: 10px 12px; border-bottom: 1px dashed #e5e7eb;\">Length</td>\n        <td style=\"padding: 10px 12px; border-bottom: 1px dashed #e5e7eb;\">5.5 ‚Äì 6.5 mm</td>\n      </tr>\n      <tr>\n        <td style=\"padding: 10px 12px; border-bottom: 1px dashed #e5e7eb;\">Width</td>\n        <td style=\"padding: 10px 12px; border-bottom: 1px dashed #e5e7eb;\">2.2 ‚Äì 2.6 mm</td>\n      </tr>\n      <tr>\n        <td style=\"padding: 10px 12px; border-bottom: 1px dashed #e5e7eb;\">Length-to-Width Ratio</td>\n        <td style=\"padding: 10px 12px; border-bottom: 1px dashed #e5e7eb;\">2.5 ‚Äì 3.0 (Avg ‚âà 2.55)</td>\n      </tr>\n      <tr>\n        <td style=\"padding: 10px 12px; border-bottom: 1px dashed #e5e7eb;\">Surface</td>\n        <td style=\"padding: 10px 12px; border-bottom: 1px dashed #e5e7eb;\">Slightly Rough, Matte to Semi-Glossy</td>\n      </tr>\n      <tr>\n        <td style=\"padding: 10px 12px; border-bottom: 1px dashed #e5e7eb;\">Raw Color</td>\n        <td colspan=\"2\" style=\"padding: 10px 12px; border-bottom: 1px dashed #e5e7eb;\">Grayish-White or Light Cream</td>\n      </tr>\n    </table>\n    <div style=\"display: flex; gap: 20px; flex-wrap: wrap; margin-top: 24px; justify-content: space-around;\">\n      <div style=\"flex: 1 1 300px; min-width: 260px;\">\n        <div style=\"font-size: 13px; color: #6b7280; margin-bottom: 8px;\">ASCII Image (Simplified View):</div>\n      </div>\n    </div>\n  </div>\n</div>\n","metadata":{}},{"cell_type":"markdown","source":"\n\n<div style=\"padding: 10px; text-align: center; height: 70px;\">\n    <div style=\"\n        padding: 0 50px;\n        margin: auto;\n        width: fit-content;\n        border-radius: 25px 50px 50px 25px;\n        background: black;\n        color: #ffffff;\n        height: 90%;\n        font-size: 30px;\n        line-height: 70px;\n        box-shadow: 0 0 8px green;\n        font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;\n        text-shadow: 1px 1px 3px black;\n    \">\n        Arborio  \n    </div>\n</div>\n\n","metadata":{}},{"cell_type":"code","source":"folder = os.path.join(dir1, 'Arborio') \n\n\nimages = [f for f in os.listdir(folder)]\n\nn_rows, n_cols = 2, 3\nn = n_rows * n_cols\n\n\nselected_images = random.sample(images, n)\n\n\nfig, ax = plt.subplots(n_rows, n_cols, figsize=(15, 8))\nfig.suptitle('Arborio', fontsize=18)\n\n\nfor idx, img_name in enumerate(selected_images):\n    row = idx // n_cols\n    col = idx % n_cols\n    img_path = os.path.join(folder, img_name)\n    img = cv2.imread(img_path)\n    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n    ax[row][col].imshow(img)\n    ax[row][col].set_title(img_name, fontsize=7)\n    ax[row][col].axis('off')\n\n\nplt.tight_layout()\nplt.show()","metadata":{"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"<div style=\"color: #111; background: #cfd3daff; border-radius: 20px; max-width: 960px; margin: 0 auto; box-shadow: 0 6px 20px rgba(15,23,42,0.08); padding: 32px; border: 1px solid #e6e6e6;\">\n  <!-- Arborio Grain Summary -->\n  <div style=\"background: #fff7e6; border-radius: 14px; padding: 22px; box-shadow: 0 6px 20px rgba(15,23,42,0.08); border: 1px solid #e6e6e6; margin-bottom: 20px;\">\n    <h1 style=\"font-size: 22px; border-bottom: 2px solid #facc15; display: inline-block; padding-bottom: 4px; margin-top: 0;\">\n      Grain Shape Summary ‚Äî <strong>Arborio</strong>\n    </h1>\n    <p style=\"margin: 6px 0 18px; color: #6b7280; font-size: 15px; line-height: 1.6;\">\n      All sources are nearly unanimous: a short, round, and plump grain with a flat tip and no curvature.\n    </p>\n    <table style=\"width: 100%; border-collapse: collapse; margin-top: 10px; font-size: 14px; border-radius: 10px; overflow: hidden;\">\n      <tr>\n        <th style=\"background: #fdfbf6; color: #374151; font-weight: 600; padding: 10px 12px; border-bottom: 1px solid #e6e6e6;\">Feature</th>\n        <th style=\"background: #fdfbf6; color: #374151; font-weight: 600; padding: 10px 12px; border-bottom: 1px solid #e6e6e6;\">Exact Value</th>\n        <th style=\"background: #fdfbf6; color: #374151; font-weight: 600; padding: 10px 12px; border-bottom: 1px solid #e6e6e6;\">Grain Type</th>\n      </tr>\n      <tr>\n        <td style=\"padding: 10px 12px; border-bottom: 1px dashed #e5e7eb;\">Overall Shape</td>\n        <td style=\"padding: 10px 12px; border-bottom: 1px dashed #e5e7eb;\">Round & Plump</td>\n        <td rowspan=\"6\" style=\"padding: 10px 12px; border-bottom: 1px dashed #e5e7eb;\">Short-Grain</td>\n      </tr>\n      <tr>\n        <td style=\"padding: 10px 12px; border-bottom: 1px dashed #e5e7eb;\">Grain Tip</td>\n        <td style=\"padding: 10px 12px; border-bottom: 1px dashed #e5e7eb;\">Rounded Tip ‚Äì No Curvature</td>\n      </tr>\n      <tr>\n        <td style=\"padding: 10px 12px; border-bottom: 1px dashed #e5e7eb;\">Length</td>\n        <td style=\"padding: 10px 12px; border-bottom: 1px dashed #e5e7eb;\">5.0 ‚Äì 6.0 mm</td>\n      </tr>\n      <tr>\n        <td style=\"padding: 10px 12px; border-bottom: 1px dashed #e5e7eb;\">Width</td>\n        <td style=\"padding: 10px 12px; border-bottom: 1px dashed #e5e7eb;\">2.8 ‚Äì 3.3 mm</td>\n      </tr>\n      <tr>\n        <td style=\"padding: 10px 12px; border-bottom: 1px dashed #e5e7eb;\">Length-to-Width Ratio</td>\n        <td style=\"padding: 10px 12px; border-bottom: 1px dashed #e5e7eb;\">1.7 ‚Äì 1.9 (Avg ‚âà 1.79‚Äì1.80)</td>\n      </tr>\n      <tr>\n        <td style=\"padding: 10px 12px; border-bottom: 1px dashed #e5e7eb;\">Surface</td>\n        <td style=\"padding: 10px 12px; border-bottom: 1px dashed #e5e7eb;\">Semi-Glossy, Pearly</td>\n      </tr>\n      <tr>\n        <td style=\"padding: 10px 12px; border-bottom: 1px dashed #e5e7eb;\">Raw Color</td>\n        <td colspan=\"2\" style=\"padding: 10px 12px; border-bottom: 1px dashed #e5e7eb;\">Pearly White</td>\n      </tr>\n    </table>\n    <div style=\"display: flex; gap: 20px; flex-wrap: wrap; margin-top: 24px; justify-content: space-around;\">\n      <div style=\"flex: 1 1 300px; min-width: 260px;\">\n        <div style=\"font-size: 13px; color: #6b7280; margin-bottom: 8px;\">ASCII Image (Simplified View):</div>\n      </div>\n    </div>\n  </div>\n</div>\n","metadata":{}},{"cell_type":"markdown","source":"\n\n<div style=\"padding: 10px; text-align: center; height: 70px;\">\n    <div style=\"\n        padding: 0 50px;\n        margin: auto;\n        width: fit-content;\n        border-radius: 25px 50px 50px 25px;\n        background: black;\n        color: #ffffff;\n        height: 90%;\n        font-size: 30px;\n        line-height: 70px;\n        box-shadow: 0 0 8px green;\n        font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;\n        text-shadow: 1px 1px 3px black;\n    \">\n        Basmati\n    </div>\n</div>\n\n","metadata":{}},{"cell_type":"code","source":"folder = os.path.join(dir1, 'Basmati') \n\n\nimages = [f for f in os.listdir(folder)]\n\nn_rows, n_cols = 2, 3\nn = n_rows * n_cols\n\n\nselected_images = random.sample(images, n)\n\n\nfig, ax = plt.subplots(n_rows, n_cols, figsize=(15, 8))\nfig.suptitle('Basmati', fontsize=18)\n\n\nfor idx, img_name in enumerate(selected_images):\n    row = idx // n_cols\n    col = idx % n_cols\n    img_path = os.path.join(folder, img_name)\n    img = cv2.imread(img_path)\n    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n    ax[row][col].imshow(img)\n    ax[row][col].set_title(img_name, fontsize=7)\n    ax[row][col].axis('off')\n\n\nplt.tight_layout()\nplt.show()","metadata":{"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"<div style=\"color: black; background: #cfd3daff; border-radius: 20px; max-width: 960px; margin: 0 auto; box-shadow: 0 6px 20px rgba(15, 23, 42, 0.08); padding: 32px; border: 1px solid #e6e6e6;\">\n\n  <div style=\"background: #fff7e6; border-radius: 14px; padding: 22px; box-shadow: 0 6px 20px rgba(15, 23, 42, 0.08); border: 1px solid #e6e6e6; margin-bottom: 20px;\">\n    <h1 style=\"font-size: 22px; border-bottom: 2px solid #facc15; display: inline-block; padding-bottom: 4px; margin-top: 0;\">\n      Grain Shape Summary ‚Äî <strong>Basmati</strong>\n    </h1>\n    <p style=\"margin: 6px 0 18px; color: #6b7280; font-size: 15px; line-height: 1.6;\">\n      All sources are nearly unanimous: a long, slender, and elongated grain with a sharp, conical tip.\n    </p>\n    <table style=\"width: 100%; border-collapse: collapse; margin-top: 10px; font-size: 14px;\">\n      <tr>\n        <th style=\"background: #fdfbf6; color: #374151; font-weight: 600; padding: 10px 12px; border-bottom: 1px solid #e6e6e6;\">Feature</th>\n        <th style=\"background: #fdfbf6; color: #374151; font-weight: 600; padding: 10px 12px; border-bottom: 1px solid #e6e6e6;\">Exact Value</th>\n        <th style=\"background: #fdfbf6; color: #374151; font-weight: 600; padding: 10px 12px; border-bottom: 1px solid #e6e6e6;\">Grain Type</th>\n      </tr>\n      <tr>\n        <td style=\"padding: 10px 12px; border-bottom: 1px dashed #e5e7eb;\">Overall Shape</td>\n        <td style=\"padding: 10px 12px; border-bottom: 1px dashed #e5e7eb;\">Slender & Elongated, Slightly Curved in the Middle</td>\n        <td rowspan=\"6\" style=\"padding: 10px 12px; border-bottom: 1px dashed #e5e7eb;\">Long-Grain</td>\n      </tr>\n      <tr>\n        <td style=\"padding: 10px 12px; border-bottom: 1px dashed #e5e7eb;\">Grain Tip</td>\n        <td style=\"padding: 10px 12px; border-bottom: 1px dashed #e5e7eb;\">Tapering Tip ‚Äì No Severe Curvature</td>\n      </tr>\n      <tr>\n        <td style=\"padding: 10px 12px; border-bottom: 1px dashed #e5e7eb;\">Length</td>\n        <td style=\"padding: 10px 12px; border-bottom: 1px dashed #e5e7eb;\">6.0 ‚Äì 8.0 mm (Minimum Standard 6.61 mm)</td>\n      </tr>\n      <tr>\n        <td style=\"padding: 10px 12px; border-bottom: 1px dashed #e5e7eb;\">Width</td>\n        <td style=\"padding: 10px 12px; border-bottom: 1px dashed #e5e7eb;\">1.5 ‚Äì 2.0 mm</td>\n      </tr>\n      <tr>\n        <td style=\"padding: 10px 12px; border-bottom: 1px dashed #e5e7eb;\">Length-to-Width Ratio</td>\n        <td style=\"padding: 10px 12px; border-bottom: 1px dashed #e5e7eb;\">> 3.0 (Average 3.5‚Äì3.7)</td>\n      </tr>\n      <tr>\n        <td style=\"padding: 10px 12px; border-bottom: 1px dashed #e5e7eb;\">Surface</td>\n        <td style=\"padding: 10px 12px; border-bottom: 1px dashed #e5e7eb;\">Smooth & Translucent, Glossy</td>\n      </tr>\n      <tr>\n        <td style=\"padding: 10px 12px; border-bottom: 1px dashed #e5e7eb;\">Raw Color</td>\n        <td colspan=\"2\" style=\"padding: 10px 12px; border-bottom: 1px dashed #e5e7eb;\">Creamy White</td>\n      </tr>\n    </table>\n    <div style=\"display: flex; gap: 20px; flex-wrap: wrap; margin-top: 24px; align-items: center; justify-content: space-around;\">\n      <div style=\"flex: 1 1 300px; min-width: 260px;\">\n        <div style=\"font-size: 13px; color: #6b7280; margin-bottom: 8px;\">ASCII Image (Simplified View):</div>\n      </div>\n    </div>\n  </div>\n\n</div>\n","metadata":{}},{"cell_type":"markdown","source":"\n\n<div style=\"padding: 10px; text-align: center; height: 70px;\">\n    <div style=\"\n        padding: 0 50px;\n        margin: auto;\n        width: fit-content;\n        border-radius: 25px 50px 50px 25px;\n        background: black;\n        color: #ffffff;\n        height: 90%;\n        font-size: 30px;\n        line-height: 70px;\n        box-shadow: 0 0 8px green;\n        font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;\n        text-shadow: 1px 1px 3px black;\n    \">\n    Jasmine\n    </div>\n</div>\n\n","metadata":{}},{"cell_type":"code","source":"folder = os.path.join(dir1, 'Jasmine') \n\n\nimages = [f for f in os.listdir(folder)]\n\nn_rows, n_cols = 2, 3\nn = n_rows * n_cols\n\n\nselected_images = random.sample(images, n)\n\n\nfig, ax = plt.subplots(n_rows, n_cols, figsize=(15, 8))\nfig.suptitle('Jasmine', fontsize=18)\n\nfor idx, img_name in enumerate(selected_images):\n    row = idx // n_cols\n    col = idx % n_cols\n    img_path = os.path.join(folder, img_name)\n    img = cv2.imread(img_path)\n    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n    ax[row][col].imshow(img)\n    ax[row][col].set_title(img_name, fontsize=7)\n    ax[row][col].axis('off')\n\n\nplt.tight_layout()\nplt.show()","metadata":{"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"<div style=\"color: black; background: #cfd3daff; border-radius: 20px; max-width: 960px; margin: 0 auto; box-shadow: 0 6px 20px rgba(15, 23, 42, 0.08); padding: 32px; border: 1px solid #e6e6e6;\">\n\n  <div style=\"background: #fff7e6; border-radius: 14px; padding: 22px; box-shadow: 0 6px 20px rgba(15, 23, 42, 0.08); border: 1px solid #e6e6e6; margin-bottom: 20px;\">\n    <h1 style=\"font-size: 22px; border-bottom: 2px solid #facc15; display: inline-block; padding-bottom: 4px; margin-top: 0;\">\n      Grain Shape Summary ‚Äî <strong>Jasmine</strong>\n    </h1>\n    <p style=\"margin: 6px 0 18px; color: #6b7280; font-size: 15px; line-height: 1.6;\">\n      All sources are nearly unanimous: a long, slender grain, slightly curved with a sharp, conical tip.\n    </p>\n    <table style=\"width: 100%; border-collapse: collapse; margin-top: 10px; font-size: 14px;\">\n      <tr>\n        <th style=\"background: #fdfbf6; color: #374151; font-weight: 600; padding: 10px 12px; border-bottom: 1px solid #e6e6e6;\">Feature</th>\n        <th style=\"background: #fdfbf6; color: #374151; font-weight: 600; padding: 10px 12px; border-bottom: 1px solid #e6e6e6;\">Exact Value</th>\n        <th style=\"background: #fdfbf6; color: #374151; font-weight: 600; padding: 10px 12px; border-bottom: 1px solid #e6e6e6;\">Grain Type</th>\n      </tr>\n      <tr>\n        <td style=\"padding: 10px 12px; border-bottom: 1px dashed #e5e7eb;\">Overall Shape</td>\n        <td style=\"padding: 10px 12px; border-bottom: 1px dashed #e5e7eb;\">Slender, Elongated, Slightly Curved</td>\n        <td rowspan=\"6\" style=\"padding: 10px 12px; border-bottom: 1px dashed #e5e7eb;\">Long-Grain</td>\n      </tr>\n      <tr>\n        <td style=\"padding: 10px 12px; border-bottom: 1px dashed #e5e7eb;\">Grain Tip</td>\n        <td style=\"padding: 10px 12px; border-bottom: 1px dashed #e5e7eb;\">Tapering/Pointed Tip</td>\n      </tr>\n      <tr>\n        <td style=\"padding: 10px 12px; border-bottom: 1px dashed #e5e7eb;\">Length</td>\n        <td style=\"padding: 10px 12px; border-bottom: 1px dashed #e5e7eb;\">7.0 ‚Äì 8.0 mm (Thailand Standard Minimum 7.0 mm)</td>\n      </tr>\n      <tr>\n        <td style=\"padding: 10px 12px; border-bottom: 1px dashed #e5e7eb;\">Width</td>\n        <td style=\"padding: 10px 12px; border-bottom: 1px dashed #e5e7eb;\">1.8 ‚Äì 2.2 mm</td>\n      </tr>\n      <tr>\n        <td style=\"padding: 10px 12px; border-bottom: 1px dashed #e5e7eb;\">Length-to-Width Ratio</td>\n        <td style=\"padding: 10px 12px; border-bottom: 1px dashed #e5e7eb;\">> 3.0 (Average 3.6‚Äì3.7)</td>\n      </tr>\n      <tr>\n        <td style=\"padding: 10px 12px; border-bottom: 1px dashed #e5e7eb;\">Surface</td>\n        <td style=\"padding: 10px 12px; border-bottom: 1px dashed #e5e7eb;\">Smooth, Glossy, Semi-Translucent</td>\n      </tr>\n      <tr>\n        <td style=\"padding: 10px 12px; border-bottom: 1px dashed #e5e7eb;\">Raw Color</td>\n        <td colspan=\"2\" style=\"padding: 10px 12px; border-bottom: 1px dashed #e5e7eb;\">White to Slightly Translucent</td>\n      </tr>\n    </table>\n    <div style=\"display: flex; gap: 20px; flex-wrap: wrap; margin-top: 24px; align-items: center; justify-content: space-around;\">\n      <div style=\"flex: 1 1 300px; min-width: 260px;\">\n        <div style=\"font-size: 13px; color: #6b7280; margin-bottom: 8px;\">ASCII Image (Simplified View):</div>\n      </div>\n    </div>\n  </div>\n\n</div>\n","metadata":{}},{"cell_type":"markdown","source":"\n\n<div style=\"padding: 10px; text-align: center; height: 70px;\">\n    <div style=\"\n        padding: 0 50px;\n        margin: auto;\n        width: fit-content;\n        border-radius: 25px 50px 50px 25px;\n        background: black;\n        color: #ffffff;\n        height: 90%;\n        font-size: 30px;\n        line-height: 70px;\n        box-shadow: 0 0 8px green;\n        font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;\n        text-shadow: 1px 1px 3px black;\n    \">\n         Karacadag  \n    </div>\n</div>\n\n","metadata":{}},{"cell_type":"code","source":"folder = os.path.join(dir1, 'Karacadag') \n\n\nimages = [f for f in os.listdir(folder)]\n\nn_rows, n_cols = 2, 3\nn = n_rows * n_cols\n\nselected_images = random.sample(images, n)\n\n\nfig, ax = plt.subplots(n_rows, n_cols, figsize=(15, 8))\nfig.suptitle('Karacadag', fontsize=18)\n\n\nfor idx, img_name in enumerate(selected_images):\n    row = idx // n_cols\n    col = idx % n_cols\n    img_path = os.path.join(folder, img_name)\n    img = cv2.imread(img_path)\n    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n    ax[row][col].imshow(img)\n    ax[row][col].set_title(img_name, fontsize=7)\n    ax[row][col].axis('off')\n\n\nplt.tight_layout()\nplt.show()","metadata":{"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"<div style=\"background: linear-gradient(180deg, #cfd3daff, #f3f7f6); padding: 20px; color: #0f172a; font-family: 'IRANSans', 'Vazirmatn', system-ui, -apple-system, 'Segoe UI', Roboto, 'Helvetica Neue', Arial;\">\n  <h1 style=\"font-size: 22px; border-bottom: 2px solid #facc15; display: inline-block; padding-bottom: 4px; margin-top: 0;\">\n    Grain Shape Summary ‚Äî <strong>Karacadag</strong>\n  </h1>\n  <p style=\"margin: 6px 0 18px; color: #6b7280; font-size: 15px; line-height: 1.6;\">\n    All sources are nearly unanimous: a long, straight grain, slightly plump in the middle with a flat, gently tapering tip.\n  </p>\n\n  <table style=\"width: 100%; border-collapse: collapse; margin-top: 10px; font-size: 14px;\">\n    <tr>\n      <th style=\"background: #fdfbf6; color: #374151; font-weight: 600; padding: 10px 12px; border-bottom: 1px solid #e6e6e6;\">Feature</th>\n      <th style=\"background: #fdfbf6; color: #374151; font-weight: 600; padding: 10px 12px; border-bottom: 1px solid #e6e6e6;\">Exact Value</th>\n      <th style=\"background: #fdfbf6; color: #374151; font-weight: 600; padding: 10px 12px; border-bottom: 1px solid #e6e6e6;\">Grain Type</th>\n    </tr>\n    <tr>\n      <td style=\"padding: 10px 12px; border-bottom: 1px dashed #e5e7eb;\">Overall Shape</td>\n      <td style=\"padding: 10px 12px; border-bottom: 1px dashed #e5e7eb;\">Elongated & Straight, Slightly Plump in the Middle</td>\n      <td rowspan=\"6\" style=\"padding: 10px 12px; border-bottom: 1px dashed #e5e7eb;\">Long-Grain</td>\n    </tr>\n    <tr>\n      <td style=\"padding: 10px 12px; border-bottom: 1px dashed #e5e7eb;\">Grain Tip</td>\n      <td style=\"padding: 10px 12px; border-bottom: 1px dashed #e5e7eb;\">Gently Tapering Tip</td>\n    </tr>\n    <tr>\n      <td style=\"padding: 10px 12px; border-bottom: 1px dashed #e5e7eb;\">Length</td>\n      <td style=\"padding: 10px 12px; border-bottom: 1px dashed #e5e7eb;\">6.0 ‚Äì 7.0 mm</td>\n    </tr>\n    <tr>\n      <td style=\"padding: 10px 12px; border-bottom: 1px dashed #e5e7eb;\">Width</td>\n      <td style=\"padding: 10px 12px; border-bottom: 1px dashed #e5e7eb;\">2.0 ‚Äì 2.3 mm</td>\n    </tr>\n    <tr>\n      <td style=\"padding: 10px 12px; border-bottom: 1px dashed #e5e7eb;\">Length-to-Width Ratio</td>\n      <td style=\"padding: 10px 12px; border-bottom: 1px dashed #e5e7eb;\">~3.0 (Average 2.95‚Äì3.02)</td>\n    </tr>\n    <tr>\n      <td style=\"padding: 10px 12px; border-bottom: 1px dashed #e5e7eb;\">Surface</td>\n      <td style=\"padding: 10px 12px; border-bottom: 1px dashed #e5e7eb;\">Smooth, Matte to Semi-Glossy</td>\n    </tr>\n    <tr>\n      <td style=\"padding: 10px 12px; border-bottom: 1px dashed #e5e7eb;\">Raw Color</td>\n      <td colspan=\"2\" style=\"padding: 10px 12px; border-bottom: 1px dashed #e5e7eb;\">White to Creamy</td>\n    </tr>\n  </table>\n\n  <div style=\"display: flex; gap: 20px; flex-wrap: wrap; margin-top: 24px; align-items: center; justify-content: space-around;\">\n    <div style=\"flex: 1 1 300px; min-width: 260px;\">\n      <div style=\"font-size: 13px; color: #6b7280; margin-bottom: 8px;\">ASCII Image (Simplified View):</div>\n    </div>\n  </div>\n</div>\n","metadata":{}},{"cell_type":"code","source":"from PIL import Image\n\nimg = Image.open('/kaggle/input/picture/xx.png')\nimg\n","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"\n<div style=\"background:#0d1117;color:#f0f6fc;font-family:Segoe UI, Tahoma,sans-serif;padding:20px;border-radius:12px;max-width:900px;margin:30px auto;box-shadow:0 6px 25px rgba(0,0,0,0.6);\">\n  <h1 style=\"color:#00d9ff;text-shadow:0 0 8px rgba(0,217,255,0.6);text-align:center;\">üèÜ Rice Type Rankings</h1>\n  <p style=\"text-align:center;color:#9ca3af;font-size:13px;\">Two criteria: Grain Length (Long ‚Üí Short) and Length-to-Width Ratio L/W (Slender ‚Üí Plump)</p>\n  <hr style=\"border:1px solid rgba(255,255,255,0.1);margin:20px 0;\">\n \n  <!-- Table 1 -->\n  <div style=\"background:#161b22;padding:15px;border-radius:10px;margin-bottom:18px;\">\n    <h2 style=\"color:#00d9ff;\">1. Ranking Based on Grain Length (Long ‚Üí Short)</h2>\n    <table style=\"width:100%;border-collapse:collapse;color:#f0f6fc;font-size:14px;\">\n      <thead>\n        <tr style=\"background:rgba(255,255,255,0.05);color:#9ca3af;\">\n          <th style=\"padding:8px;\">Rank</th>\n          <th style=\"padding:8px;\">Rice Type</th>\n          <th style=\"padding:8px;\">Average Length (mm)</th>\n          <th style=\"padding:8px;\">Note</th>\n        </tr>\n      </thead>\n      <tbody>\n        <tr><td style=\"text-align:center;color:#8b5cf6;\">1</td><td>Jasmine</td><td>7.44</td><td style=\"color:#9ca3af;\">Source: Provided data</td></tr>\n        <tr><td style=\"text-align:center;color:#8b5cf6;\">2</td><td>Basmati</td><td>7.02</td><td style=\"color:#9ca3af;\">Measured on 100 images</td></tr>\n        <tr><td style=\"text-align:center;color:#8b5cf6;\">3</td><td>Karacadag</td><td>6.52</td><td style=\"color:#9ca3af;\">Measured on 100 images</td></tr>\n        <tr><td style=\"text-align:center;color:#8b5cf6;\">4</td><td>Ipsala</td><td>5.92</td><td style=\"color:#9ca3af;\">Measured on 100 images</td></tr>\n        <tr><td style=\"text-align:center;color:#8b5cf6;\">5</td><td>Arborio</td><td>5.61</td><td style=\"color:#9ca3af;\">Measured on 100 images</td></tr>\n      </tbody>\n    </table>\n    <p style=\"color:#9ca3af;font-size:13px;\">üîπ Note: Order based on provided average length values (Long ‚Üí Short).</p>\n  </div>\n \n  <!-- Table 2 -->\n  <div style=\"background:#161b22;padding:15px;border-radius:10px;\">\n    <h2 style=\"color:#00d9ff;\">2. Ranking Based on L/W Ratio (Slender ‚Üí Plump)</h2>\n    <table style=\"width:100%;border-collapse:collapse;color:#f0f6fc;font-size:14px;\">\n      <thead>\n        <tr style=\"background:rgba(255,255,255,0.05);color:#9ca3af;\">\n          <th style=\"padding:8px;\">Rank</th>\n          <th style=\"padding:8px;\">Rice Type</th>\n          <th style=\"padding:8px;\">L/W (Average)</th>\n          <th style=\"padding:8px;\">Status</th>\n        </tr>\n      </thead>\n      <tbody>\n        <tr><td style=\"text-align:center;color:#8b5cf6;\">1</td><td>Basmati</td><td>3.71</td><td style=\"color:#9ca3af;\">Slenderest</td></tr>\n        <tr><td style=\"text-align:center;color:#8b5cf6;\">2</td><td>Jasmine</td><td>3.67</td><td style=\"color:#9ca3af;\">Very Slender</td></tr>\n        <tr><td style=\"text-align:center;color:#8b5cf6;\">3</td><td>Karacadag</td><td>3.02</td><td style=\"color:#9ca3af;\">Long but Slightly Plump</td></tr>\n        <tr><td style=\"text-align:center;color:#8b5cf6;\">4</td><td>Ipsala</td><td>2.56</td><td style=\"color:#9ca3af;\">Medium, Plump</td></tr>\n        <tr><td style=\"text-align:center;color:#8b5cf6;\">5</td><td>Arborio</td><td>1.80</td><td style=\"color:#9ca3af;\">Plumpest</td></tr>\n      </tbody>\n    </table>\n    <p style=\"color:#9ca3af;font-size:13px;\">üîπ Note: Higher L/W value indicates a slenderer grain.</p>\n  </div>\n  <div style=\"margin-top:18px;color:#9ca3af;font-size:13px;\">\n    <strong>üìù General Note:</strong>\n    <ul style=\"margin:8px 0 0 20px;line-height:1.6;\">\n      <li>Numbers were extracted and sorted from your data.</li>\n      <li>If desired, I can create these tables in CSV or JSON format.</li>\n    </ul>\n  </div>\n</div>","metadata":{}},{"cell_type":"code","source":"\n\n# Count number of images in each class\ncounts = []\nfor class_name in detection:\n    class_path = os.path.join(dir1, class_name)\n    counts.append(len(os.listdir(class_path)))\n\n# Color palette for both charts\ncolors = ['#1A3636', '#407F7F', '#699B9B', '#9EC2C2', '#CFE5E5']\n\n# ===== Create two subplots side by side =====\nfig, axes = plt.subplots(1, 2, figsize=(16, 6))\n\n# =================== 1) Barplot ===================\nsns.barplot(\n    y=detection,\n    x=counts,\n    palette=colors[:len(counts)],\n    ax=axes[0]\n)\n\n# Add numeric labels to bars\nfor i, p in enumerate(axes[0].patches):\n    width = p.get_width()\n    axes[0].text(\n        width + 5,\n        p.get_y() + p.get_height()/2.,\n        str(counts[i]),\n        va=\"center\",\n        fontsize=9,\n        fontweight=\"bold\"\n    )\n\n# Set barplot titles and labels\naxes[0].set_title(\"Number of Images per Class\", fontsize=16, fontweight=\"bold\")\naxes[0].set_xlabel(\"Number of Samples\")\naxes[0].set_ylabel(\"Classes\")\naxes[0].grid(axis='x', linestyle='--', alpha=0.6)\n\n# =================== 2) Pie Donut Chart ===================\n# Create pie chart\nwedges, texts, autotexts = axes[1].pie(\n    counts,\n    labels=detection,\n    autopct='%1.1f%%',\n    pctdistance=0.8,\n    colors=colors[:len(counts)],\n    textprops={'fontsize': 10}\n)\n\n# Create donut hole\ncentre_circle = plt.Circle((0,0), 0.55, fc='white')\naxes[1].add_artist(centre_circle)\n\n# Set title\naxes[1].set_title(\"Class Distribution (Donut Chart)\", fontsize=16, fontweight=\"bold\")\n\n# Adjust layout\nplt.tight_layout()\n\n# Show both plots\nplt.show()\n","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"\n \n<div style=\"\n    background: black;\n    width: 90%;\n    margin: 30px auto;\n    border-radius: 20px;\n    text-align: center;\n    box-shadow: 0 0 5px cyan;\n    color:blue;\n    font-size: 26px;\n    padding: 20px 30px;\n    font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;\n    font-weight: bold;\n\">\n CNN model\n</div>\n","metadata":{}},{"cell_type":"code","source":"my_list = []\nfor class_name in detection :\n    class_path = os.path.join(dir1, class_name)\n    files = os.listdir(class_path)\n    for file in files :\n        file_path = os.path.join(class_path, file)\n        my_list.append((file_path, class_name))\n\n# Convert the list to a pandas DataFrame\ndf = pd.DataFrame(my_list, columns=['file_path', 'label'])\n\n# Shuffle the dataframe rows\ndf = df.sample(frac=1).reset_index(drop=True)\n\ndf.head()","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# split DataFrame to train, vlidation and test DataFrames\ntrain_df = df[:60000]\nvalid_df = df[60000:67500]\ntest_df = df[67500:]\n\n# Reset indexes of DataFrames\ntrain_df.reset_index(inplace = True, drop = True)\ntest_df.reset_index(inplace = True, drop = True)\nvalid_df.reset_index(inplace = True, drop = True)\n\nprint(colored(f'Number of samples in train DataFrame : {len(train_df)}', 'green', attrs=['bold']))\nprint(colored(f'Number of samples in validation DataFrame : {len(valid_df)}', 'green', attrs=['bold']))\nprint(colored(f'Number of samples in test DataFrame : {len(test_df)}', 'green', attrs=['bold']))","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"<div style=\"\n  display: flex;\n  justify-content: center;\n  align-items: center;\n  margin-top: 20px;\n  margin-bottom: 20px;\n\">\n  <div style=\"\n    width: 200px;\n    padding: 15px 20px;\n    border-radius: 12px;\n    font-weight: bold;\n    font-size: 18px;\n    color: #fff;\n    background: linear-gradient(135deg, #6a11cb 0%, #2575fc 100%);\n    box-shadow: 0 4px 15px rgba(0,0,0,0.3);\n    text-align: center;\n    transition: transform 0.3s, box-shadow 0.3s;\n    cursor: pointer;\n  \">\n   base\n  </div>\n</div>\n","metadata":{}},{"cell_type":"code","source":"# Set the image size and batch size\nimage_size = (224, 224)\nbatch_size = 16\n\n# Create an ImageDataGenerator object with rescale options for image preprocessing\ndatagen = ImageDataGenerator(\n    rescale=1./255\n)\n\n\n# Create a generator for the training data\nTrain = datagen.flow_from_dataframe(\n    train_df,\n    x_col='file_path',\n    y_col='label',\n    target_size=image_size,\n    batch_size=batch_size,\n    class_mode='categorical',\n    shuffle=True\n)\n\n# Create a generator for the validation data\nValidation = datagen.flow_from_dataframe(\n    valid_df,\n    x_col='file_path',\n    y_col='label',\n    target_size=image_size,\n    batch_size=batch_size,\n    class_mode='categorical',\n    shuffle=False\n)\n\n# Create a generator for the test data\nTest = datagen.flow_from_dataframe(\n    test_df,\n    x_col='file_path',\n    y_col='label',\n    target_size=image_size,\n    batch_size=batch_size,\n    class_mode='categorical',\n    shuffle=False\n)","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"input_shape = (224, 224, 3)\n\ncnn_base = Sequential()\ncnn_base.add(Conv2D(32, 3, activation='relu', input_shape=input_shape))\ncnn_base.add(MaxPooling2D(2, 2))\ncnn_base.add(keras.layers.BatchNormalization())\ncnn_base.add(Conv2D(64, 3, activation='relu'))\ncnn_base.add(MaxPooling2D(2, 2))\n\ncnn_base.add(Flatten())\ncnn_base.add(Dense(512, activation='relu'))\ncnn_base.add(Dense(64, activation='relu'))\ncnn_base.add(Dense(5, activation='softmax'))\nprint(\"ok\")","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import warnings\nwarnings.filterwarnings('ignore')\nmodel_path = \"/kaggle/input/model1/tensorflow2/default/1/my_model.h5\"\ncnn = tf.keras.models.load_model(model_path)\nkeras.utils.plot_model(\n    cnn,                        # An instance of a Keras model\n    to_file=\"model.png\",          # \"name.format\" to specify the saved file name\n    show_shapes=True,            # Option to display shape information\n    show_dtype=False,             # Option to display layer data types\n    show_layer_names=True,       # Option to display names of layers\n    rankdir=\"TB\",                 # A parameter for the PyDot library\n    expand_nested=False,          # Option to expand nested Functional models into clusters\n    dpi=200,                      # Resolution of the image in dots per inch\n    show_layer_activations=True, # Option to show layer activations\n    show_trainable=False) ","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"cnn_base.compile(optimizer='adam',loss='categorical_crossentropy',metrics=['accuracy'])\nprint(\"ok\")","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import warnings\nimport pickle\nwarnings.filterwarnings('ignore')\n\nhistory_base = cnn_base.fit(\n    x=Train,\n    validation_data=Validation,\n    epochs=10\n)\n\nwith open(\"training_history.pkl\", \"wb\") as f:\n    pickle.dump(history_base.history, f)\n\nprint(\"ok‚úÖ\")\n","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"plt.figure(figsize=(15, 5))\nplt.plot(cnn_base.history.history['accuracy'], \n         marker='o', color='#FF6B6B', linewidth=2, markersize=8, label='Train Accuracy')\nplt.plot(cnn_base.history.history['val_accuracy'], \n         marker='*', color='#1DD1A1', linewidth=2, markersize=10, label='Validation Accuracy')\n\nplt.title('üìä Model Accuracy', fontsize=16, fontweight='bold')\nplt.xlabel('Epoch', fontsize=12)\nplt.ylabel('Accuracy', fontsize=12)\nplt.legend(loc='lower right', frameon=True, shadow=True)\nplt.grid(True, linestyle='--', alpha=0.7)\nplt.show()\n\n# -----------------------------\n# Loss\n# -----------------------------\nplt.figure(figsize=(15, 5))\nplt.plot(cnn_base.history.history['loss'], \n         marker='o', color='#FF9F43', linewidth=2, markersize=8, label='Train Loss')\nplt.plot(cnn_base.history.history['val_loss'], \n         marker='*', color='#48DBFB', linewidth=2, markersize=10, label='Validation Loss')\n\nplt.title('üî• Model Loss', fontsize=16, fontweight='bold')\nplt.xlabel('Epoch', fontsize=12)\nplt.ylabel('Loss', fontsize=12)\nplt.legend(loc='upper right', frameon=True, shadow=True)\nplt.grid(True, linestyle='--', alpha=0.7)\nplt.show()\n","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"y_pred = cnn_base.predict(Test, verbose=1)\ny_pred_bool = np.argmax(y_pred, axis=1)\n\nclf_report = classification_report(Test.classes, y_pred_bool, target_names=detection)\n\nprint(clf_report)","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def plot_confusion_matrix(y_test, y_pred, labels):\n    cm = confusion_matrix(y_test, y_pred)\n\n    cm_normalized = cm.astype(\"float\") / cm.sum(axis=1)[:, np.newaxis]\n\n    annot_text = np.empty_like(cm).astype(str)\n\n    for i in range(cm.shape[0]):\n        for j in range(cm.shape[1]):\n            annot_text[i, j] = f\"{cm[i, j]}\\n({cm_normalized[i, j]*100:.1f}%)\"\n\n    plt.figure(figsize=(8, 6))\n    ax = sns.heatmap(cm,\n                     annot=annot_text,\n                     fmt='',\n                     cmap=\"Blues\")\n\n    ax.set_xlabel('Predicted labels', fontsize=18)\n    ax.set_ylabel('True labels', fontsize=18)\n    ax.set_title('Confusion Matrix (Count + Percentage)', fontsize=22)\n\n    ax.xaxis.set_ticklabels(labels)\n    ax.yaxis.set_ticklabels(labels)\n\n    plt.show()\nlabels = list(Test.class_indices.keys())\n\nplot_confusion_matrix(Test.classes, y_pred_bool, labels)\n","metadata":{"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":" <div style=\"\n  display: flex;\n  justify-content: center;\n  align-items: center;\n  margin-top: 20px;\n  margin-bottom: 20px;\n\">\n  <div style=\"\n    width: 200px;\n    padding: 15px 20px;\n    border-radius: 12px;\n    font-weight: bold;\n    font-size: 18px;\n    color: #fff;\n    background: linear-gradient(135deg, #6a11cb 0%, #2575fc 100%);\n    box-shadow: 0 4px 15px rgba(0,0,0,0.3);\n    text-align: center;\n    transition: transform 0.3s, box-shadow 0.3s;\n    cursor: pointer;\n  \">\n    architecture 2\n  </div>\n</div>\n","metadata":{}},{"cell_type":"code","source":"# Set the image size and batch size\nimage_size = (224, 224)\nbatch_size = 16\n\n# Create an ImageDataGenerator object with rescale options for image preprocessing\ndatagen = ImageDataGenerator(\n    rescale=1./255\n)\n\n\n# Create a generator for the training data\nTrain_2 = datagen.flow_from_dataframe(\n    train_df,\n    x_col='file_path',\n    y_col='label',\n    target_size=image_size,\n    batch_size=batch_size,\n    class_mode='categorical',\n    shuffle=True\n)\n\n# Create a generator for the validation data\nValidation_2 = datagen.flow_from_dataframe(\n    valid_df,\n    x_col='file_path',\n    y_col='label',\n    target_size=image_size,\n    batch_size=batch_size,\n    class_mode='categorical',\n    shuffle=False\n)\n\n# Create a generator for the test data\nTest_2 = datagen.flow_from_dataframe(\n    test_df,\n    x_col='file_path',\n    y_col='label',\n    target_size=image_size,\n    batch_size=batch_size,\n    class_mode='categorical',\n    shuffle=False\n)","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow.keras import layers, models\n\nnum_classes = 5  \nbase_model = tf.keras.applications.Xception(\n    weights=\"imagenet\",\n    include_top=False,\n    input_shape=(224, 224, 3)\n)\n\nbase_model.trainable = False\n\nx = layers.GlobalAveragePooling2D()(base_model.output)\nx = layers.Dense(1024, activation=\"relu\")(x)\noutput = layers.Dense(num_classes, activation=\"softmax\")(x)\n\n\nmodel = base_model.Model(inputs=base_model.input, outputs=output)\n\nmodel.compile(\n    optimizer=tf.keras.optimizers.Adam(learning_rate=1e-4),\n    loss=\"sparse_categorical_crossentropy\",\n    metrics=[\"accuracy\"]\n)\n\n\nprint(\"ok\")","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# # 1. Train\n# history = model.fit(Train_3, validation_data=Validation_3, epochs=10)\n\n# # 2. Save files\n# import pickle\n# with open(\"training_history.pkl\", \"wb\") as f:\n#     pickle.dump(history.history, f)\n\n# model.save(\"my_model_3.h5\")\n\n# # 3. Direct Download Links\n# from IPython.display import FileLink\n\n# print(\"Download Model:\")\n# display(FileLink('my_model_3.h5'))\n\n# print(\"Download History:\")\n# display(FileLink('training_history.pkl'))\n","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# import os\n# import random\n# import numpy as np\nfrom tensorflow.keras.preprocessing import image\nfrom tensorflow.keras.applications.xception import preprocess_input, decode_predictions\n# model = tf.keras.applications.Xception(weights=\"imagenet\")  # ŸÜŸÖŸàŸÜŸá\n# last_conv_layer_name = \"block14_sepconv2_act\"  # ŸÜÿßŸÖ ÿ¢ÿÆÿ±€åŸÜ ŸÑÿß€åŸá ⁄©ÿßŸÜŸàŸÑŸàÿ¥ŸÜ€å ÿØÿ± Xception\n\n# 2. Final list of images (one image from each folder)\nselected_images = []\n\nfor class_name in detection:\n    folder_path = f'/kaggle/input/rice-image-dataset/Rice_Image_Dataset/{class_name}'\n    img_files = os.listdir(folder_path)\n    \n    # Randomly select one image\n    chosen_img = random.choice(img_files)\n    img_path = os.path.join(folder_path, chosen_img)\n    selected_images.append(img_path)\n\n# Display paths of selected images\nfor img in selected_images:\n    print(img)\n\nprint(\"Total selected images:\", len(selected_images))\n","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"img_size = (224,224)  \n\n# ================================\n# 4. Helper functions\n# ================================\ndef get_img_array(img_path, size=(224, 224)):\n    \"\"\"Load image and convert to model input array\"\"\"\n    img = image.load_img(img_path, target_size=size)\n    array = image.img_to_array(img)\n    array = np.expand_dims(array, axis=0)\n    return preprocess_input(array)\n\ndef make_gradcam_heatmap(img_array, model, last_conv_layer_name):\n    \"\"\"Generate Grad-CAM heatmap for an image\"\"\"\n    grad_model = keras.models.Model(\n        [model.inputs], [model.get_layer(last_conv_layer_name).output, model.output]\n    )\n    with tf.GradientTape() as tape:\n        conv_outputs, predictions = grad_model(img_array)\n        pred_index = tf.argmax(predictions[0])\n        loss = predictions[:, pred_index]\n\n    grads = tape.gradient(loss, conv_outputs)\n    pooled_grads = tf.reduce_mean(grads, axis=(0, 1, 2))\n    conv_outputs = conv_outputs[0]\n    heatmap = conv_outputs @ pooled_grads[..., tf.newaxis]\n    heatmap = tf.squeeze(heatmap)\n    heatmap = tf.maximum(heatmap, 0) / tf.math.reduce_max(heatmap)\n    return heatmap.numpy()\n\nimport cv2  #  resize heatmap\n\ndef overlay_heatmap(img_path, heatmap, alpha=0.4):\n    \"\"\"Overlay Grad-CAM heatmap on original image with color\"\"\"\n    # Load original image\n    img = image.load_img(img_path, target_size=img_size)\n    img = image.img_to_array(img) / 255.0\n    \n    # Resize heatmap to match image size\n    heatmap_resized = cv2.resize(heatmap, (img.shape[1], img.shape[0]))\n    \n    # Apply colormap (e.g., jet)\n    from matplotlib import cm\n    heatmap_color = cm.jet(heatmap_resized)  # RGBA\n    heatmap_color = heatmap_color[..., :3]   # Keep only RGB channels\n    \n    # Superimpose\n    superimposed_img = img * (1 - alpha) + heatmap_color * alpha\n    superimposed_img = np.clip(superimposed_img, 0, 1)  # Ensure valid range\n    return superimposed_img\nprint(\"ok‚úÖ\")","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ================================\n# 0. Imports\n# ================================\nimport os\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport tensorflow as tf\nfrom tensorflow.keras import layers, models\nfrom tensorflow.keras.preprocessing import image\nfrom tensorflow.keras.applications.xception import Xception, preprocess_input\nimport cv2\nfrom matplotlib import cm\n\n# ================================\n# 1. Load or define model\n# ================================\nnum_classes = 5\nimg_size = (224, 224)\nclass_names = ['Karacadag', 'Basmati', 'Jasmine', 'Arborio', 'Ipsala']  # Example, replace with your classes\n\n# Load pretrained model from file\nmodel_path = \"/kaggle/input/xcp/tensorflow2/default/1/my_model_3.h5\"\nmodel = tf.keras.models.load_model(model_path)\n\n# ================================\n# 2. Helper functions\n# ================================\n\ndef get_img_array(img_path, size=(224, 224)):\n    \"\"\"Load image and convert to model input array\"\"\"\n    img = image.load_img(img_path, target_size=size)\n    array = image.img_to_array(img)\n    array = np.expand_dims(array, axis=0)\n    return preprocess_input(array)\n\ndef find_last_conv_layer(model):\n    \"\"\"Automatically find the last convolutional layer (with Activation if exists)\"\"\"\n    for layer in reversed(model.layers):\n        if isinstance(layer, (tf.keras.layers.Conv2D, tf.keras.layers.SeparableConv2D)):\n            next_idx = model.layers.index(layer) + 1\n            if next_idx < len(model.layers):\n                next_layer = model.layers[next_idx]\n                if isinstance(next_layer, tf.keras.layers.Activation):\n                    return next_layer.name\n            return layer.name\n    raise ValueError(\"No convolutional layer found in the model.\")\n\ndef make_gradcam_heatmap(img_array, model, last_conv_layer_name):\n    \"\"\"Generate Grad-CAM heatmap for an image\"\"\"\n    grad_model = tf.keras.models.Model(\n        [model.inputs],\n        [model.get_layer(last_conv_layer_name).output, model.output]\n    )\n    with tf.GradientTape() as tape:\n        conv_outputs, predictions = grad_model(img_array)\n        pred_index = tf.argmax(predictions[0])\n        loss = predictions[:, pred_index]\n\n    grads = tape.gradient(loss, conv_outputs)\n    pooled_grads = tf.reduce_mean(grads, axis=(0, 1, 2))\n    conv_outputs = conv_outputs[0]\n    heatmap = conv_outputs @ pooled_grads[..., tf.newaxis]\n    heatmap = tf.squeeze(heatmap)\n    heatmap = tf.maximum(heatmap, 0) / tf.math.reduce_max(heatmap)\n    return heatmap.numpy()\n\ndef overlay_heatmap(img_path, heatmap, alpha=0.4):\n    \"\"\"Overlay Grad-CAM heatmap on original image\"\"\"\n    img = image.load_img(img_path, target_size=img_size)\n    img = image.img_to_array(img) / 255.0\n    heatmap_resized = cv2.resize(heatmap, (img.shape[1], img.shape[0]))\n    heatmap_color = cm.jet(heatmap_resized)[..., :3]\n    superimposed_img = img * (1 - alpha) + heatmap_color * alpha\n    superimposed_img = np.clip(superimposed_img, 0, 1)\n    return superimposed_img\n","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\n# ================================\n# 3. Grad-CAM visualization\n# ================================\nlast_conv_layer_name = find_last_conv_layer(model)\nprint(\"Last convolutional layer for Grad-CAM:\", last_conv_layer_name)\n\n# List of images to process\n# selected_images = [\"/kaggle/input/xcp/images/example1.jpg\", \"/kaggle/input/xcp/images/example2.jpg\"]  # Replace with your paths\n\nfor img_path in selected_images:\n    img_file = os.path.basename(img_path)\n    \n    # 3.1 Preprocess image\n    img_array = get_img_array(img_path, size=img_size)\n    \n    # 3.2 Predict\n    preds = model.predict(img_array)[0]\n    pred_class = class_names[np.argmax(preds)]\n    pred_prob = np.max(preds)\n    \n    print(f\"\\nPredictions for {img_file}:\")\n    for i, prob in enumerate(preds):\n        print(f\"{class_names[i]}: {prob:.4f}\")\n    print(f\"Predicted class: {pred_class} ({pred_prob:.4f})\")\n    \n    # 3.3 Generate Grad-CAM\n    heatmap = make_gradcam_heatmap(img_array, model, last_conv_layer_name)\n    superimposed_img = overlay_heatmap(img_path, heatmap)\n    \n    # 3.4 Plot original and Grad-CAM side by side\n    fig, axs = plt.subplots(1, 2, figsize=(12, 5))\n    original_img = image.load_img(img_path, target_size=img_size)\n    axs[0].imshow(original_img)\n    axs[0].axis('off')\n    axs[0].set_title(\"Original Image\")\n    \n    axs[1].imshow(superimposed_img)\n    axs[1].axis('off')\n    axs[1].set_title(f\"Grad-CAM\\nPrediction: {pred_class} ({pred_prob:.2f})\")\n    \n    plt.suptitle(img_file, fontsize=14)\n    plt.tight_layout()\n    plt.show()\n","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport pickle\n\n# -----------------------------\n# Load training history from a pickle file\n# -----------------------------\nwith open(\"/kaggle/input/4/tensorflow2/default/1/training_history (1).pkl\", \"rb\") as f:\n    history = pickle.load(f)\n\n# -----------------------------\n# General settings for nicer plots\n# -----------------------------\nplt.style.use('seaborn-v0_8-darkgrid')  # Use seaborn darkgrid style\nplt.rcParams['figure.figsize'] = [10, 5]  # Default figure size\nplt.rcParams['font.size'] = 12  # Default font size\n\n# -----------------------------\n# üìà Plot training & validation accuracy\n# -----------------------------\nplt.figure(figsize=(15, 5))\nplt.plot(history['accuracy'], \n         marker='o', color='#FF6B6B', linewidth=2, markersize=8, label='Train Accuracy')  # Training accuracy\nplt.plot(history['val_accuracy'], \n         marker='*', color='#1DD1A1', linewidth=2, markersize=10, label='Validation Accuracy')  # Validation accuracy\n\n# Highlight a specific point (epoch 3)\n\n\nplt.title('üìä Model Accuracy', fontsize=16, fontweight='bold')\nplt.xlabel('Epoch', fontsize=12)\nplt.ylabel('Accuracy', fontsize=12)\nplt.legend(loc='lower right', frameon=True, shadow=True)\nplt.grid(True, linestyle='--', alpha=0.7)\nplt.show()\n\n# -----------------------------\n# üìâ Plot training & validation loss\n# -----------------------------\nplt.figure(figsize=(15, 5))\nplt.plot(history['loss'], \n         marker='o', color='#FF9F43', linewidth=2, markersize=8, label='Train Loss')  # Training loss\nplt.plot(history['val_loss'], \n         marker='*', color='#48DBFB', linewidth=2, markersize=10, label='Validation Loss')  # Validation loss\n\n\n\nplt.title('üî• Model Loss', fontsize=16, fontweight='bold')\nplt.xlabel('Epoch', fontsize=12)\nplt.ylabel('Loss', fontsize=12)\nplt.legend(loc='upper right', frameon=True, shadow=True)\nplt.grid(True, linestyle='--', alpha=0.7)\nplt.show()\n","metadata":{},"outputs":[],"execution_count":null}]}